# Introduction to Algorithmic Efficiency (Complexity) 📚 

Now that we have developed an understanding of how some searching & sorting algorithms work, the next logical step is to examine just how well they work. In this section we will analyse the performance of algorithms. In computer science this is often referred to as algorithmic efficiency or complexity.


Two common measures of algorithmic efficiency are space and time.

**1. Space:** Provides an indication of the demands an algorithm places on memory in terms of space requirements.

**2. Time:**  Provides an indication of the time requirements of an algorithm.

For the most part, we will be confining the remainder of our discussion to time complexity.

## How to Compare Algorithms 👨🏽‍🔬
Before we can begin to compare algorithms in terms of their performance, we must first devise _(or at least agree upon)_ some system that is both impartial and reliable.

On the surface it might seem fair and make sense to simply time how long it takes an algorithm to run in minutes and seconds _(or milliseconds)_ and use this as a measure of performance. As it turns out however this would be neither fair nor reliable. 

This is because a computer’s performance can depend on a variety of different factors _(e.g. processor clock speed, word size, bus width and amount of available memory)_, and so, an algorithm that takes **1000 milliseconds** to run on one computer might run in just **10 milliseconds** on another _(one hundred times faster!)_. 

In fact, depending on the processor load, the time taken to run an algorithm could potentially vary significantly from run to run on the same processor.

Also, the running time of an algorithm is likely to vary in accordance with the size of its input. Intuitively it is easy to understand that a particular sorting algorithm will sort **1,000** integers must faster than it will sort **1,000,000** integers. 

However, as we will soon learn to appreciate _(hopefully!)_, it is the specific techniques and _**nuances**_ employed by algorithms that have a much greater bearing on performance than the size of the input.

Finally, there are questions such as:
  - What is the fastest time an algorithm can run in? _(i.e. what is the best case performance?)_
  - Is there an average performance time for a particular algorithm?
  - What about a worst case?

As it turns out it is this final question _(regarding worst case)_ that computer scientists are most interested in. 

The reason is that a _**worst case running time**_ gives users a _**bottom line guarantee**_ that an algorithm will finish at worst within a particular timeframe, and for this reason worst case scenario is used as a metric for comparing algorithms.

## Big O 📚👨🏽‍💻
Big O is a notation used in Computer Science to describe the worst case running time _(or space requirements)_ of an algorithm in terms of the size of its input usually denoted by ``𝑛``.

![image](https://github.com/ross-bish/Algorithms/assets/83789503/a7b51721-258c-4318-b8f9-84d1a1198941)

Big O notation is important for several reasons:

- Big O Notation helps analyze the efficiency of algorithms.
- It provides a way to describe how the runtime or space requirements of an algorithm grow as the input size increases.
- Allows programmers to compare different algorithms and choose the most efficient one for a specific problem.
- Helps in understanding the scalability of algorithms and predicting how they will perform as the input size grows.
- Enables developers to optimize code and improve overall performance.

_**Bad = 🟥**_ 

_**Ok = 🟨**_

_**Good = 🟩**_

Let's take a look at some common examples of **Big O** values:

### _O( 1 )_ 🟩
An algorithm described in this manner will always run within some constant time _(sometimes called bounded time)_ regardless of the size of the input.
Such algorithms are said to take _‘order of 1’_, or ``O(1)`` time to complete.

To take an analogy, let’s say it’s the weekend and you were preparing to do some serious study but before you get started you first need to clear your room/desk. The time required to do this work doesn’t depend in any way on the number of subjects you intend to study. It will be completed within some **constant amount of time** regardless of whether you will study two or ten subjects.

### _O(n)_ 🟨
If the length of time it takes to run an algorithm _increases_ in proportion to the size of the input the algorithm is said to run in _linear time._ Such algorithms have an ``𝑂(𝑛)`` complexity.

The _**linear (sequential) search**_ algorithm used to find some target value (argument) in a list that contains ``𝑛`` values is a classic example of an ``𝑂(𝑛)`` algorithm. This is because in the worst case scenario every element in the list will have to be examined in order to find the target value.

These algorithms are characterised by the following loop structure.
````python
for i in range(n):
    print(i) # this line will be executed n times
````

Lets keep going with our earlier analogy – let ``n`` be the number of subjects you are going to study and let us say that you had decided to allocate a fixed amount of time to each subject. It makes sense therefore that the more subjects you study the longer it will take to finish your study. Twice as many subjects will require twice the amount of time!

### _O(n²)_ 🟥
Now let’s say that you decided to use a slightly different approach to your study. Instead of allocating the same fixed amount of time to each subject you decide to allocate fixed units of time to reading individual pages of notes. You start by reading one page for the first subject, two for the second, three for the third and so on. By the time you have reached your ``nth`` subject you will need to read ``𝑛`` pages of notes. The amount of time it takes to complete your study in this case is known as quadratic time and is written as ``O(𝑛²)``.

Algorithms of this type are characterised by _loops nested to one level._ 

  - For each of the ``𝑛`` iterations carried out by the outer loop, the inner loop will perform ``𝑛`` iterations of its own. 
  - This is illustrated in the snippet of Python code below in which the print statement appears within a nested for loop and will be executed ``𝑛²`` times.

````python
for i in range(n):
    for j in range(n):
      print(i, j) # this line will be executed n squared times
````

#### 💡Note:
Remember the three elementary sort algorithms 

- Selection sort
- Insertion sort
- Bubble sort
  
These are all examples of algorithms whose time complexity is **quadratic** `O(𝑛²)`. 

Furthermore, it is noteworthy that algorithms in this class are **impractical** to use when it comes to dealing with large volumes of data. 

Just think about it – if the size of a list **doubles** it would take **four times longer** to sort; increasing the size of a list threefold will result in a nine-fold increase in time!!!


### _O(log₂n)_ 🟩
These algorithms are said to be logarithmic. 

For algorithms that have logarithmic time complexity it means that as the value `𝑛` increases, the time complexity of your program increases by a _logarithmic factor._

Such algorithms are characterised by _cutting the size of the input in half_ in each step as it moves towards a solution. 
Take for example the following analysis of a **Binary search:**

![image](https://github.com/ross-bish/Algorithms/assets/83789503/dd7abd1f-96e3-47dc-b92d-ba946d7a1208)

As can be seen from the table above the maximum number of comparisons _(steps)_ the binary search algorithm needs to perform in order to find some target value just increases by one each time the size of the input list is doubled.

#### 💡Note: 
The relationship between the size of the input ``𝑛`` and the maximum number of comparisons ``𝑐`` required is given by: 

![image](https://github.com/ross-bish/Algorithms/assets/83789503/35341789-979b-4030-ab75-604c3d3af697)

Therefore...

![image](https://github.com/ross-bish/Algorithms/assets/83789503/fa7c0144-03ea-4c94-997f-2276dd5c882f)

So binary search has ``𝑂(log₂𝑛)`` time complexity which is a very impressive and desirable feature for any algorithm to have. 

More importantly however is the fact that we can use this to calculate the maximum number of comparisons it will take to search a list of any size _(i.e. we can guarantee an upper bound)_ 

- For example a list with ``2³⁰ ≈ 1 𝑏𝑖𝑙𝑙𝑖𝑜𝑛`` elements will take no more than ``31`` comparisons.
- This is very useful information for software designers to have at hand when they need to choose the most appropriate algorithm for the system they are working on.

### _O(nlog₂n)_ 🟥
When it comes to analysing worst case time complexity of algorithms that sort by using a series of head-to-head comparisons, it is a proven fact that the best we can hope to achieve is ``𝑂(𝑛 log2𝑛)``, also called _**linearithmic**_ time. 

- _**Linearithmic**_ complexity lies somewhere between _linear_ and _quadratic_. 
- It is not an understatement to say that algorithms with this class of time complexity result in seismic improvements in performance.

_**Linearithmic**_ algorithms are again characterised by the _divide-and-conquer_ approach to problem solving. 

- Depending on the particular algorithm there will be ``𝑛`` divisions and each division will take log ``𝑛`` steps to conquer or vice versa.
- Two examples of algorithms that fall into this class of time complexity are **Quicksort** and **Merge sort**.

### _Intractable Problems: 0(2ⁿ) and 0(n!)_ 🟥
Finally, it is worth noting that algorithms can have time complexities that are exponential, ``𝑂(2ⁿ)`` and even worse, factorial, ``𝑂(𝑛!)``. 

The solution to the _**Travelling Salesman Problem**_ is an example of a _**factorial time**_ algorithm. 
- Algorithms of this nature are said to be intractable as their running time makes them infeasible even for very small values of ``n``.
- _Intractable problems are problems for which there exist no efficient algorithms to solve them._
