# Introduction to Algorithmic Efficiency (Complexity) üìö 

Now that we have developed an understanding of how some search/sort algorithms work, the next logical step is to examine just how well they work. In this section we will analyse the performance of algorithms. In computer science this is often referred to as algorithmic efficiency or complexity.


Two common measures of algorithmic efficiency are space and time.

**1. Space:** Provides an indication of the demands an algorithm places on memory in terms of space requirements.

**2. Time:**  Provides an indication of the time requirements of an algorithm.

For the most part, we will be confining the remainder of our discussion to time complexity.

## How to Compare Algorithms üë®üèΩ‚Äçüî¨
Before we can begin to compare algorithms in terms of their performance, we must first devise _(or at least agree upon)_ some system that is both impartial and reliable.

On the surface it might seem fair and make sense to simply time how long it takes an algorithm to run in minutes and seconds _(or milliseconds)_ and use this as a measure of performance. As it turns out however this would be neither fair nor reliable. 

This is because a computer‚Äôs performance can depend on a variety of different factors _(e.g. processor clock speed, word size, bus width and amount of available memory)_, and so, an algorithm that takes **1000 milliseconds** to run on one computer might run in just **10 milliseconds** on another _(one hundred times faster!)_. 

In fact, depending on the processor load, the time taken to run an algorithm could potentially vary significantly from run to run on the same processor.

Also, the running time of an algorithm is likely to vary in accordance with the size of its input. Intuitively it is easy to understand that a particular sorting algorithm will sort **1,000** integers must faster than it will sort **1,000,000** integers. 

However, as we will soon learn to appreciate _(hopefully!)_, it is the specific techniques and _**nuances**_ employed by algorithms that have a much greater bearing on performance than the size of the input.

Finally, there are questions such as:
  - What is the fastest time an algorithm can run in _(i.e. what is the best case performance)_?
  - Or is there an average performance time for a particular algorithm?
  - What about a worst case?

As it turns out it is this final question _(regarding worst case)_ that computer scientists are most interested in. 

The reason is that a _**worst case running time**_ gives users a _**bottom line guarantee**_ that an algorithm will finish at worst within a particular timeframe, and for this reason worst case scenario is used as a metric for comparing algorithms.

## Big O üìöüë®üèΩ‚Äçüíª
Big O is a notation used in Computer Science to describe the worst case running time _(or space requirements)_ of an algorithm in terms of the size of its input usually denoted by ``ùëõ``.

![image](https://github.com/ross-bish/Algorithms/assets/83789503/a7b51721-258c-4318-b8f9-84d1a1198941)

Big O notation is important for several reasons:

- Big O Notation is important because it helps analyze the efficiency of algorithms.
- It provides a way to describe how the runtime or space requirements of an algorithm grow as the input size increases.
- Allows programmers to compare different algorithms and choose the most efficient one for a specific problem.
- Helps in understanding the scalability of algorithms and predicting how they will perform as the input size grows.
- Enables developers to optimize code and improve overall performance.


